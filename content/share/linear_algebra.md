+++
date = '2025-02-26T20:48:12+08:00'
draft = false
title = '时隔十年，AI时代重新学习线性代数'
+++

## 为什么重新开始学习线性代数?
从小数学就比较一般，一直不太喜欢数学，2015年，我大学一年级，线性代数成了我数学课的“开门红”。说实话，那会儿我完全懵了，满脑子都是问号：这矩阵算来算去到底有啥用？为啥非得折腾这些数字方块？考试前拼命刷题，考完试立马把书扔一边，心里还嘀咕着“这辈子估计用不上了”。可谁能想到，十年后的今天，我居然又翻开了线性代数的课本，而且这次是心甘情愿的！为啥？因为AI火了，而我发现，当年那些让我头大的矩阵运算，现在居然成了理解人工智能的“基本功”。从机器学习到图像识别，线性代数无处不在。

## 矩阵运算
你可以把矩阵想象成一个数据表格，每一行代表一个样本，每一列代表一个特征。比如，你想训练一个AI识别猫和狗，矩阵里的数据可能就是一堆猫狗图片的像素值。深度学习的过程，本质上就是对这些矩阵进行各种加减乘除、变形和组合，最终让AI学会从数据中提取规律。比如，神经网络里的每一层都在做矩阵乘法，通过调整矩阵里的参数（也就是权重），AI就能慢慢学会区分猫和狗。

## 使用Python的Numpy对矩阵进行运算
当然，我们使用Numpy这个强大的库就能实现矩阵的运算，不需要再手动进行计算。

安装numpy
```
pip3 install numpy
```


导入numpy模块
```
import numpy as np
```
声明矩阵
```
matrix1 = np.array([[1, 2], [3, 4]])
```

_________________


### 矩阵相加
```
matrix1 = np.array([[1, 2, 3], [3, 4, 9]])
matrix2 = np.array([[5, 6], [7, 8], [3, 5, 6]])
print(matrix1 + matrix2)
```
matrix1的列数必须和matrix2的行数相等，矩阵相加是机器学习中的基本操作之一，广泛应用于特征工程、模型训练、深度学习、数据增强等领域。


_________________


### 矩阵相减
```
matrix1 = np.array([[1, 2], [3, 4]])
matrix2 = np.array([[5, 6], [7, 8]])
print(matrix1 - matrix2)
```
矩阵相减在机器学习中主要用于计算差异、误差、残差等，是优化模型、分析数据和实现特定任务（如图像处理、异常检测）的重要工具.

_________________


### 转置矩阵
```
matrix1 = np.array([[1, 2, 3], [4, 5, 6]])
```
```
matrix1T = matrix1.T
```
或者
```
matrix1T = np.transpose(matrix1)
```
转置矩阵是线性代数中的一个基本操作，它将矩阵的行和列互换。矩阵的转置操作可以类比为将图片翻转一下，但具体来说，它更像是将图片沿着对角线进行翻转。

_________________


### 矩阵相乘
```
matrix1 = np.array([[1, 2], [3, 4], [3, 4]]) # 3X2
matrix2 = np.array([[5, 6], [5, 6]]) # 2X2
print(np.shape(matrix1), np.shape(matrix2))
print(np.dot(matrix1, matrix2))
```
为了乘法可以定义，矩阵1的列数必须和矩阵2的行数相等才能相乘在机器学习中非常重要，常用于神经网络的前向传播、线性回归等任务。

_________________


### 单位矩阵和逆矩阵
矩阵和逆矩阵的乘积为单位矩阵
```
# 定义一个单位矩阵
matrix1 = np.eye(3)
print(matrix1)

# 计算逆矩阵
matrix2 = np.array([[4, 7],
              [2, 6]])
matrix2_inv = np.linalg.inv(matrix2)
# 验证乘积
print(np.dot(matrix2, matrix2_inv))
```

_________________


### 线性相关
线性相关：向量组中存在“冗余”，即至少有一个向量可以被其他向量通过缩放和相加（线性组合）表示出来。
线性无关：向量组中没有“冗余”，每个向量都提供了新的方向信息。
例子:
* 在二维空间中：
    * 两个向量线性相关：它们在同一条直线上。
    * 两个向量线性无关：它们不在同一条直线上。

* 在三维空间中：
    * 三个向量线性相关：它们在同一平面上。
    * 三个向量线性无关：它们不在同一平面上。
```
# 代码计算向量是否线性相关
v1 = np.array([1, 2])
v2 = np.array([2, 4])
# 组成矩阵
A = np.array([v1, v2])
# 计算行列式
det = np.linalg.det(A)
print(det)
```

_________________


### 生成子空间
指的是一组向量通过线性组合所能生成的所有向量的集合
生成子空间可以理解为：
* 给定一组向量，通过它们的缩放和相加（线性组合），能够“覆盖”的所有可能的向量。
* 这些向量“张成”的空间可以是直线、平面、三维空间，甚至更高维的空间。

_________________


### 范数

$用来衡量向量或矩阵“大小”或“长度”的一种函数，对于一个向量x=(x1,x2,x3,,,xn)，范数记为\Vert x \Vert
常见范数$：
* $ {L^1}范数（曼哈顿范数）:\|\mathbf{x}\|_1 = \sum_{i=1}^n |x_i|$ 
* $ {L^2}范数（欧几里得范数）:\|\mathbf{x}\|_2 = \sqrt{\sum_{i=1}^n x_i^2} $
* $ {L^∞}范数:\|\mathbf{x}\|_\infty = \max_{i} |x_i|
 $

```
# 定义向量
x = np.array([1, 2, 3])

# 计算 L1 范数
l1_norm = np.linalg.norm(x, ord=1)
print("L1 范数：", l1_norm)

# 计算 L2 范数
l2_norm = np.linalg.norm(x, ord=2)
print("L2 范数：", l2_norm)

# 计算 L∞ 范数
linf_norm = np.linalg.norm(x, ord=np.inf)
print("L∞ 范数：", linf_norm)
```

